I'm n*** going to give y*** t*** more example problems of computing. eigendecomposition of matrices by hand. Obviously this is on t*** computer b*** I encourage y*** to do these problems by hand a*** they're n*** done in Matlab. So I guess that's what it means by hand. So let's g*** started. Here is a t*** by t*** matrix a*** y*** should pause t*** video go through find t*** eigenvalues a*** t*** eigenvectors of this matrix so t*** w*** we start is by shifting t*** matrix by Lambda so subtracting lambda from t*** diagonal elements setting t*** determinant equal to zero a*** then proceeding to compute t*** determinant which is also called t*** characteristic equation of this matrix. So that works o*** to be three minus lambda times s*** minus lambda minus four equals zero a*** then expanding these t*** terms multiplying o*** these t*** terms a*** then collecting t*** like terms gives us lambda squared minus nine lambda plus fourteen equals zero a*** then this expression here c*** be factored into lambda minus seven a*** Lambda minus t*** a*** n*** it's pretty easy to s*** that t*** t*** lambdas that will solve this equation that solve this equation. In other words t*** t*** eigenvalues of this matrix a*** plus 7 a*** plus two. So that w*** step o*** we found t*** eigenvalues of this matrix a*** n*** we go through f*** each of these eigenvalues shift t*** matrix by that amount by that value a*** then figure o*** what is a vector in t*** shifted matrices null space. OK so let's start with 2. So this gives us t*** matrix 1 1 4 4. A*** n*** we want to find t*** missing vector here a*** you've probably guessed it already it is 1 minus 1. A*** of course y*** know that also acceptable would be minus 1 1 or minus quadrillion plus quadrillion. This vector simply identifies a subspace f*** t*** null space of this shifted matrix in a*** vector that's in that null space is perfectly fine as an eigenvector. N*** that said t*** best choice f*** an eigenvector would be a vector that h*** a norm of 1 a magnitude of 1. A*** t*** second best choice would be integer values that a*** easy to interpret a*** compact to write like this. A*** right. So this w*** f*** two. N*** we go to t*** second eigenvalues which w*** seven a*** that gives us this matrix minus 4 1 a*** 4 minus 1. A*** here a vector that could work as an eigenvector as a basis f*** t*** null space of this shifted matrix is t*** numbers 1 4. So here is t*** b*** picture overview we have this matrix a*** here y*** s*** t*** eigenvalue a*** i*** corresponding eigenvector a*** t*** other eigenvalue a*** t*** other corresponding eigenvector. N*** your results will be correct. If y*** have t*** correct pairing of eigenvalue a*** a*** multiple a*** scaled version of this vector; it doesn't matter if y*** call this lambda o*** a*** this V o*** that's fine because there's no intrinsic ordering. What matters is that y*** have t*** pairing correct. A*** right. So that w*** t*** t*** by t*** case. N*** let's go f*** a three by three case. This o*** is a little b*** more challenging a*** I have to admit. So I first came up with these numbers a*** then I started computing t*** eigendecomposition. N*** I have to admit that I g*** stuck on o*** of t*** eigenvectors. I couldn't quite figure it o*** on my own. So I used Matlab to compute that eigenvector. So what I encourage y*** to do is find a*** three eigenvalues by hand a*** I think it will be pretty obvious when y*** start working through it which is t*** difficult eigenvalue. So then what y*** should do by hand is find t*** of t*** eigenvectors that y*** c*** g*** basically just by kind of eyeballing a*** making some educated guesses. A*** then t*** third eigenvector y*** c*** u*** a computer to solve or y*** c*** just wait a*** watch me come up with a solution. A*** right. So again we start by shifting this matrix by minus lambda setting t*** determinant equal to zero a*** then proceeding to compute t*** determinant of this equation a*** altogether that gives us t*** characteristic equation of this matrix. N*** this is a little b*** longer. It's a little b*** trickier t*** arithmetic gets a little b*** hairy in particular y*** e*** up with this minus lambda cubed term as well as a couple of multiple terms with Lambda a*** Lambda squared. N*** once y*** collect a*** of these like terms you'll e*** up with an expression that looks like this. So it should be minus lambda cubed plus 10 times lambda squared plus eleven lambda equals zero N*** something interesting h*** happened here a*** of these terms have a lambda attached to them which means that we c*** take a lambda o*** of each of these terms a*** rewrite this expression as minus lambda times. A*** of this stuff a*** immediately that tells us that lambda equals zero is a solution. So when y*** s*** this lambda to be zero it doesn't actually matter what's inside this parenthetical statement that's immediately going to s*** to make this equation true. That means that o*** of t*** eigenvalues of this matrix is zero a*** I'm going to have an entire video just about this phenomenon a little b*** later in this section. B*** essentially when an eigenvector is an eigenvalue is zero. It means that t*** matrix is singular a*** that y*** c*** actually s*** by looking at this matrix a*** y*** s*** that column 1 plus column 2 equals column 3 so whenever y*** have a singular matrix at least o*** eigenvalues value is going to be equal to zero. A*** in fact t*** number of eigenvalues that a*** equal to zero tells y*** about t*** rank of this matrix. More on that in t*** later video. N*** once you've gotten to this step y*** c*** further factor this equation a*** y*** e*** up with t*** result that lambda equals zero lambda equals minus 1 a*** Lambda equals eleven. So y*** c*** probably guess that this is going to be t*** tricky I c*** value to compute t*** corresponding I c*** vector of a*** this is t*** o*** that I g*** a little b*** stuck with. A*** so I used Matlab as a crutch. I cheated a little bit. OK so b*** let's go through a*** of these. So we start with zero a*** n*** this is kind of a funny thing because we a*** shifting t*** matrix by zero which actually means we're n*** changing t*** matrix at all. A*** that means that this matrix A already h*** a non-trivial null space even without doing a*** shifting. So this problem actually boils down to finding a vector in t*** null space or a basis f*** t*** null space. Even without doing a*** shifting so based on what I just told y*** about h*** do I s*** up this matrix that column o*** plus column t*** equals column three a basis f*** t*** null space is 1 o*** minus 1. So y*** c*** t*** f*** each of these rows t*** first column plus t*** second column minus t*** third column equals zero a*** right. So n*** let's move on. So n*** we shift this matrix by eleven a*** this is basically where I g*** stuck a*** switch to matlab. So it turns o*** that eigenvector is nineteen forty o*** a*** thirty six. So if y*** figured o*** this I c*** vector on your o*** without using a computer then good f*** y*** y*** a*** a better or at least more patient mathematician than I am. A*** then we g*** to t*** third eigenvalue which w*** minus one. So n*** that becomes plus one. A*** here is that shifted matrix a*** this o*** y*** should be able to solve on your own. In fact it's even easier than it looks. A*** if y*** need a hint before I show t*** answer then t*** hint is just consider that this third column is actually pretty useless if y*** g*** r*** of this third column it becomes really easy to find t*** eigenvector in this shifted matrices null space. So in fact it's o*** minus o*** a*** then zero y*** just s*** t*** third element to be zero. So that leads us to t*** b*** picture overview of t*** eigendecomposition of this three by three singular matrix rank 2 matrix we have eigenvalues 0 minus 11 a*** minus 1. A*** these a*** t*** corresponding eigenvectors. A*** notice I've written them as r*** vectors a*** then transpose. So these a*** still column vectors we generally always think about eigenvectors as column vectors a*** y*** will learn more about w*** that is t*** case in t*** video on diagonalization, which is coming up soon.